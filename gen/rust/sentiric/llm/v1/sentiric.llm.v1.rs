// @generated
// This file is @generated by prost-build.
// =============================================================================
// RPC MESAJ TANIMLARI
// =============================================================================

/// ESKİ: LLMLocalServiceGenerateStreamRequest
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct LlamaGenerateStreamRequest {
    #[prost(string, tag="1")]
    pub system_prompt: ::prost::alloc::string::String,
    #[prost(string, tag="2")]
    pub user_prompt: ::prost::alloc::string::String,
    #[prost(string, optional, tag="3")]
    pub rag_context: ::core::option::Option<::prost::alloc::string::String>,
    #[prost(message, repeated, tag="4")]
    pub history: ::prost::alloc::vec::Vec<ConversationTurn>,
    #[prost(message, optional, tag="5")]
    pub params: ::core::option::Option<GenerationParams>,
}
/// ESKİ: LLMLocalServiceGenerateStreamResponse
#[derive(Clone, PartialEq, Eq, Hash, ::prost::Message)]
pub struct LlamaGenerateStreamResponse {
    #[prost(oneof="llama_generate_stream_response::Type", tags="1, 2")]
    pub r#type: ::core::option::Option<llama_generate_stream_response::Type>,
}
/// Nested message and enum types in `LlamaGenerateStreamResponse`.
pub mod llama_generate_stream_response {
    #[derive(Clone, PartialEq, Eq, Hash, ::prost::Oneof)]
    pub enum Type {
        /// UTF-8 string olarak token
        #[prost(string, tag="1")]
        Token(::prost::alloc::string::String),
        #[prost(message, tag="2")]
        FinishDetails(super::FinishDetails),
    }
}
// =============================================================================
// YARDIMCI MESAJ TANIMLARI
// =============================================================================

#[derive(Clone, PartialEq, Eq, Hash, ::prost::Message)]
pub struct ConversationTurn {
    /// "user", "assistant", "system"
    #[prost(string, tag="1")]
    pub role: ::prost::alloc::string::String,
    #[prost(string, tag="2")]
    pub content: ::prost::alloc::string::String,
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenerationParams {
    #[prost(int32, optional, tag="1")]
    pub max_new_tokens: ::core::option::Option<i32>,
    #[prost(float, optional, tag="2")]
    pub temperature: ::core::option::Option<f32>,
    #[prost(int32, optional, tag="3")]
    pub top_k: ::core::option::Option<i32>,
    #[prost(float, optional, tag="4")]
    pub top_p: ::core::option::Option<f32>,
    #[prost(float, optional, tag="5")]
    pub repetition_penalty: ::core::option::Option<f32>,
    #[prost(string, repeated, tag="6")]
    pub stop_sequences: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    #[prost(int64, optional, tag="7")]
    pub seed: ::core::option::Option<i64>,
    /// Motor spesifik ekstra parametreler (örn: mirostat)
    #[prost(message, optional, tag="20")]
    pub engine_specific_params: ::core::option::Option<::prost_types::Struct>,
}
#[derive(Clone, PartialEq, Eq, Hash, ::prost::Message)]
pub struct FinishDetails {
    /// "stop", "length", "error"
    #[prost(string, tag="1")]
    pub finish_reason: ::prost::alloc::string::String,
    #[prost(int32, tag="2")]
    pub prompt_tokens: i32,
    #[prost(int32, tag="3")]
    pub completion_tokens: i32,
}
// =============================================================================
// RPC MESAJ TANIMLARI
// =============================================================================

#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenerateDialogStreamRequest {
    /// örn: "llama-3-8b", "gpt-4"
    #[prost(string, tag="1")]
    pub model_selector: ::prost::alloc::string::String,
    #[prost(string, tag="2")]
    pub tenant_id: ::prost::alloc::string::String,
    /// Llama.cpp motoruna özgü parametreler
    /// DÜZELTME: Tip adı LlamaGenerateStreamRequest olarak güncellendi
    #[prost(message, optional, tag="10")]
    pub llama_request: ::core::option::Option<LlamaGenerateStreamRequest>,
}
#[derive(Clone, PartialEq, Eq, Hash, ::prost::Message)]
pub struct GenerateDialogStreamResponse {
    /// DÜZELTME: Tip adı LlamaGenerateStreamResponse olarak güncellendi
    #[prost(message, optional, tag="10")]
    pub llama_response: ::core::option::Option<LlamaGenerateStreamResponse>,
}
include!("sentiric.llm.v1.tonic.rs");
// @@protoc_insertion_point(module)